---
title: "ChurnRateClassificationModel"
author: "Matthew Li"
output:
  html_document:
    df_print: paged
  md_document:
    variant: markdown_github
---

``` {r}
library(tidyverse)
library(car)
library(ggplot2)
library(DataExplorer)
library(pastecs)
library(plyr)
library(effects)
library(tseries)
library(AER)
library(broom)
library(caret)
library(mice)
library(leaps)
```

## Introduction

To begin, I wanted to explore a dataset that is something I believe might be relevant in a business setting. I found a dataset on Kaggle about telco churn rate, which is a classic example of how to answer a business problem. 

```{r}
## load dataset
telcodata <- read.table("C:/Users/Matt/Downloads/telco_customer_churn.csv", header=TRUE, sep = ",", stringsAsFactors = TRUE)

## convert remaining categorical columns to factors
telcodata$SeniorCitizen <- as.factor(telcodata$SeniorCitizen)

## Replace "No internet service" to "No" as they generally mean the same thing
telcodata[telcodata == "No internet service"] <- "No"

## Omitting NA for now to run Boruta Algorithm

telcodata_nona <- na.omit(telcodata)

str(telcodata_nona)

```
## Problem 1

## Part a)

```{r}
## boruta algorithm to find best predictors
library(Boruta)
set.seed(123)
telcodata_boruta <- Boruta(Churn ~., data = telcodata_nona)
telcodata_boruta2 <- TentativeRoughFix(telcodata_boruta)
print(telcodata_boruta2)

## plot importance of each variable obtained from boruta 
plot(telcodata_boruta2, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(telcodata_boruta2$ImpHistory),function(i)
telcodata_boruta2$ImpHistory[is.finite(telcodata_boruta2$ImpHistory[,i]),i])
names(lz) <- colnames(telcodata_boruta2$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(telcodata_boruta2$ImpHistory), cex.axis = 0.7)

```


The top 10 predictors from the Boruta algorithm are; 
1) tenure
2) Contract
3) TotalCharges
4) MonthlyCharges
5) OnlineSecurity
6) InternetService
7) TechSupport
8) OnlineBackup
9) DeviceProtection
10) PaymentMethod

## Part b)

```{r}

## Mallows Cp test for which terms to keep in model

ss = regsubsets(Churn ~ tenure + Contract + TotalCharges + MonthlyCharges + OnlineSecurity + InternetService + TechSupport + OnlineBackup + DeviceProtection + PaymentMethod, method=c("exhaustive"), nbest = 1, nvmax = 13, data = telcodata_nona)


subsets(ss,min.size = 8, statistic="cp", legend=interactive())

```


Keep all 10 top predictors obtained from Boruta Algorithm (no subset of these 10 variables have a lower Mallows Cp)


## Part c)

I will work with the Top 7 predictors obtained in the Boruta algorithm;

1) tenure
2) Contract
3) TotalCharges
4) MonthlyCharges
5) OnlineSecurity
6) InternetService
7) TechSupport

```{r}
## keep variables desired
telcodata2 <- subset(telcodata, select = c("tenure", "Contract", "TotalCharges", "MonthlyCharges", "OnlineSecurity", "InternetService", "TechSupport", "Churn"))

```

## Problem 2

## Part a)

```{r}
## Histogram
plot_histogram(telcodata2)

```


```{r}
## QQ plot of model
plot_qq(telcodata2, ncol = 4)

```


```{r}
## correlation plot of variables

plot_correlation(telcodata2, geom_text_args = list(check_overlap = TRUE))

```


As expected, many of our categorical variables have collinearity since some (like monthly and total charges) measure similar things and some answer choices (no internet, and no) mean the same thing.

```{r}
## boxplot of variables
plot_boxplot(telcodata2, by = "Churn")

```


Just visually, we can see some interesting relationship between tenure length and churn. Seems to me like a lower tenure may lead to higher churn rate.


## Part b)

```{r}
## density plot of variables
plot_density(telcodata2)

## barplot of categorical variables
plot_bar(telcodata2)

```


One thing I noticed was that TotalCharges seems to have some right skew. In addition, this was when I realized that I was working with a binary dependent variable.

## Part c)

```{r}
###Using Box-Cox we can determine the best transformation for each non-indicator variable

## MonthlyCharges
symbox(telcodata2$MonthlyCharges, powers = c(-1,-.5,0,0.5,1))

## tenure
symbox(telcodata2$tenure, powers = c(-1,-.5,0,0.5,1))

## TotalCharges
symbox(telcodata2$TotalCharges, powers = c(-1,-.5,0,0.5,1))

```


No transformation is needed for MonthlyCharges and tenure. A sqrt transformation may be better for TotalCharges.

Transforming non-linear variables is important to ensure accuracy of the model throughout the range of possible x values.

## Part d)

Since many of our variables are categorical data, we won't really find outliers in the traditional sense there. In regards to TotalCharges, we can see that there is some right skew in the density plot.


## Part e)

```{r}
### impute missing values
## Note: for random missing values, we could use various impute methods, MICE, AMELIA, Mean/median, but since our missing values are simply because no payment has been made(new customer <1 month tenure), we simply need to replace NA with 0.

telcodata2[is.na(telcodata2)] <- 0 
                                    
```


As noted in the comments above, we simply impute the missing values with 0 because all of our NA's are not random, but actually because the tenure length is <1 month, meaning no charges were made yet. The dataset had NA's for TotalCharges = 0 so we simply replace the NA's with 0.


### Problem 3

```{r}
## initial model with predictor variables

telcomodel1 <- glm(Churn ~ tenure + Contract + TotalCharges + MonthlyCharges + OnlineSecurity + InternetService + TechSupport, data = telcodata2, family = "binomial")
summary(telcomodel1)


```


```{r}
## testing transformations

telcomodel2 <- glm(Churn ~ tenure + Contract + sqrt(TotalCharges) + MonthlyCharges + OnlineSecurity + InternetService + TechSupport, data = telcodata2, family = "binomial")
summary(telcomodel2)

```

                                        
Since the transformed model has a lower AIC it is likely a better model than the untransformed model. However, given we have a logistical regression and the interpretability of the coefficents may prove difficult with an additional sqrt transformation, we will stick with linear as the difference is not much.

```{r}
## model with statistically insignificant variables removed

telcomodel1 <- glm(Churn ~ tenure + Contract + TotalCharges  + OnlineSecurity + InternetService + TechSupport, data = telcodata2, family = "binomial")
summary(telcomodel1)
```


```{r}
## Testing for multicollinearity, 1 is none, 1<x<5 is moderate >5 is high

vif(telcomodel1)

```


As we can see, tenure and TotalCharges are colinear, we will remove TotalCharges.


```{r}
## model with statistically insignificant and colinear variables removed

telcomodel1.adj <- glm(Churn ~ tenure + Contract  + OnlineSecurity + InternetService + TechSupport, data = telcodata2, family = "binomial")
summary(telcomodel1.adj)
## assessing multicollinearity after removal
vif(telcomodel1.adj)
```

As far as I am aware, we do not need to test for heteroskedasticity because we have a logistic regression and there is no assumption of equal variance. The data is assumed to be distributed as a binomial where variance is determined by the mean. Similarly, residuals and errors cannot really be interpreted and assumed normal in the traditional sense either. Therefore, Cook's Distance would not really be relevant here.

I included code of how I would calculate Cook's Distance and remove influential outliers if this was a normal linear regression model at the bottom of the project.


```{r} 
## testing for power terms and possible interaction effects

telcomodel1.interactions <- glm(Churn ~ (tenure + Contract  + OnlineSecurity + InternetService + TechSupport)^2, data = telcodata2, family = "binomial")
summary(telcomodel1.interactions)

```


Two statistically significant interaction effects are Contract:TechSupport and OnlineSecurity:TechSupport. Lets build them into our model.


```{r}
## model with interaction effects and statistically insignificant effects removed

telcomodel1.adj.interaction <- glm(Churn ~ tenure + Contract  + OnlineSecurity + InternetService + TechSupport + Contract:TechSupport, data = telcodata2, family = "binomial")
summary(telcomodel1.adj.interaction)
```

```{r}
## AIC and BIC of model with and without interaction effects
AIC(telcomodel1.adj, telcomodel1.adj.interaction)
BIC(telcomodel1.adj, telcomodel1.adj.interaction)

```


Since the model with interactions has a higher BIC, but lower AIC, we are not sure if the interaction effects helped improve the model. Then for interpretability sake, we will stick with the model with no interactions.

```{r}
## anova test to see significance of variables on resid.dev
anova(telcomodel1.adj, test="Chisq")

```


As we can see, tenure, Contract, and InternetService has a large effect on Churn. All variables are statistically significant however, and do have atleast some effect on Churn rate.

```{r}
##bootstrapping

library(boot)  
logit_test <- function(d,indices) {  
d <- d[indices,]  
fit <- glm(Churn ~ tenure + Contract  + OnlineSecurity + InternetService + TechSupport, data = d, family = "binomial")  
return(coef(fit))  
}
boot_fit <- boot(  
   data = telcodata2, 
   statistic = logit_test, 
   R = 1000
) 

boot_fit


```

```{r}
## change index to see plot for different coefficients (should appear normal and linear)
plot(boot_fit, index=2)
```


Distribution of bootstrap correlation coefficients seem pretty normal; quantiles of standard normal linear as well which is good.

```{r}
## change index to see 95% boot.ci interval for the different coefficients
boot.ci(boot_fit,index=8, type="norm")
```


All of our bootstrapped coefficients remain significant (strictly positive or negative) except intercept which is fine. This shows our model is robust and likely not overspecified.


```{r}
### Splitting 20% of data set to become test data
n = nrow(telcodata2)

train_index = sample(n, floor(.8 * n))

train_data1 = telcodata2[train_index,]

test_data1 = telcodata2[-train_index,]

```

```{r}
### 5-fold cross-validation test

## Define Training Control

set.seed(123)
train.control <- trainControl(method = "cv", number = 5)

## Train the model
telcomodel1.adj.train <- train(Churn ~ tenure + Contract  + OnlineSecurity + InternetService + TechSupport, data = test_data1, method = "glm", family="binomial", trControl = train.control)

## Summarize the results
print(telcomodel1.adj.train)
```


Since our classifier of Churn (yes or no) is imbalanced as seen from the barplot above (~2000 yes ~5000 no), it is likely good to look at both the accuracy and the Kappa. Without diving too deep into the interpretation (I don't believe I am quite capable enough to accurately give a robust and correct interpretation), generally a Kappa of .4 and above is fair to good performance and given our accuracy of around 78% we are going to be pretty okay with our model here as a student.



```{r}
## Chosen model
summary(telcomodel1.adj)

```


## Interpretation of model (log odds)

### Holding all else constant;

Tenure: For every month increase in tenure, the odds of churn decrease by 2.81%

Contract: As opposed to month-by-month contracts, a One Year contract decreases churn rate by 52.54%.
  - Similarly, as opposed to month-by-month contracts, a two year contract decreases churn rate by        77.49%.

OnlineSecurity: As opposed to not having Online Security, a customer with OnlineSecurity decreases        churn rate by 37.84%.

InternetService: As opposed to DSL, customers with Fiber Optics have a 195.63% higher churn rate. 
  - On the other hand, as opposed to DSL, customers with no internet service have a 72.99% decrease in     churn rate.
  
TechSupport: As opposed to not having Tech Support, customers with TechSupport have a 29.84% decrease in churn rate.


### Conclusion

Biggest takeaways from building and interpreting this model is that:

1) Customers with Fiber Optics have **HUGELY** higher churn rate and as a Telco business, being aware of that may prove very profitable.

2) Offering TechSupport and OnlineSecurity pay significant dividends in keeping churn rate lower.

3) Unsurprisingly, One-Year and Two-Year contracts have significantly lower churn rates than    month-by-month contracts.

4) The longer your customer has been your customer (tenure), the less likely they are to switch providers (churn).

Touching back on what I said in the introduction, I really wanted to work with this dataset to get a feel on how to discover actionable insights from data analysis in a practical setting. Customer churn seems very relevant to a business and being able to determine some patterns in customer churn rate would bring value to the company.


This was the first time I have worked with Logistical Regression and I don't believe we learned about it in class yet so this was a fun and difficult project for me to take. Given that it is a logistical regression and not a linear regression, I read that many of the validation tests such as tests for heteroskedasticity and looking at outliers are not exactly applicable to this model. However, I did not want to start over and explore a dataset that I wasn't as passionate about and I definitely did not want to back down simply because I have never done this process before. However, since many of the questions asked about things like Cooks Distance and heteroskedasticity, I included some code I would write for linear models so it doesn't look like I am trying to take an easy way out.



```{}
## How I would find influential outliers for linear model (docreg is model)

# Find influential outliers using Cook's Distance
influencePlot(docreg)
cooksd <- cooks.distance(docreg)
plot(cooksd)
abline(h = 4/(nrow(docmodel)-11), col = "red")
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/(nrow(docmodel)), names(cooksd),""), col="red")
# Removing influential outliers using threshold of 4/N-k-1
influential.doc <- as.numeric(names(cooksd)[(cooksd > 4/(nrow(docmodel)))])
# New data set without outliers
docmod.adj <- docmodel[-c(influential.doc),] 

```

```{}
## How I would test for heteroskedasticity

library(lmtest)
bptest(model)

## How I would resolve 
library(sandwich)
coeftest(model, vcov = vcovHC(model, "HC1")

## since sample is large we could then work with those White standard errors
```