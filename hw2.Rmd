---
title: "hw2"
output:
  pdf_document: default
  html_document: default
---
``` {r}
library(tidyverse)
library(car)
library(ggplot2)
library(DataExplorer)
library(pastecs)
library(plyr)
library(effects)
library(tseries)
library(AER)
library(broom)
library(leaps)
library(caret)
library(mice)
```

```{r}
## load dataset
housepricemodel_orig <- read.table("C:/Users/Matt/Downloads/train.csv", header=TRUE, sep = ",")

```


### Problem 1)

10 variables chosen: 
  
  1) GarageArea
  2) LotArea
  3) TotRmsAbvGrd
  4) Neighborhood
  5) OpenPorchSF
  6) YearRemodAdd
  7) GrLivArea
  8) TotalBsmtSF
  9) OverallQual
  10) CentralAir
  
  
    I picked these 10 because I believed they would give a good overall measure of how houses are priced; size of various areas of the house and lot, rooms, year it was constructed/remodelled, location, and overall quality. TotRmsAbvGrd and GrLivArea gives us an idea of the size of the house, which may be an important factor in house prices. OpenPorchSF to see if how large the front porch area is affects house prices; similarly, TotalBsmtSF too see if a large basement affects prices, GarageArea for garage size, and LotArea for lot size. Neighborhood gives us an idea of location of house within Ames, which may also be an important factor. YearRemodAdd gives us an idea of the year it was built/remodelled. CentralAir to see the effect that functionality has on willingness to pay. And finally, OverallQual gives us a clearer picture on the overall material and finish of the house. I picked these 10 variables to hopefully give us a round, complete picture of the house and all factors that could affect the price.

```{r}

## keep variables desired
hpmodel <- subset(housepricemodel_orig, select = c("GarageArea", "LotArea", "TotRmsAbvGrd", "Neighborhood", "OpenPorchSF", "YearRemodAdd", "GrLivArea", "TotalBsmtSF", "OverallQual", "CentralAir", "SalePrice"))

### Change categorical variables to factor variables for - Neighborhood, CentralAir

## Analyzing Neighborhood vs price to see if I can simplify the variable into bins/fewer parts
Neighborhood.id <- revalue(hpmodel$Neighborhood, c("Blmngtn" = "1", "Blueste" = "2", "BrDale" = "3", "BrkSide" = "4", "ClearCr" = "5", "CollgCr" = "6", "Crawfor" = "7", "Edwards" = "8", "Gilbert" = "9", "IDOTRR" = "10", "MeadowV" = "11", "Mitchel" = "12", "NAmes" = "13", "NoRidge" = "14", "NPkVill" = "15", "NridgHt" = "16", "NWAmes" = "17", "OldTown" = "18", "SWISU" = "19", "Sawyer" = "20", "SawyerW" = "21", "Somerst" = "22", "StoneBr" = "23", "Timber" = "24", "Veenker" = "25"))

## CentralAir
hpmodel$CentralAir <- revalue(hpmodel$CentralAir, c("N" = "0", "Y" = "1"))

### Change variables to factor
Neighborhood.id<- as.factor(Neighborhood.id)
hpmodel$CentralAir <- as.factor(hpmodel$CentralAir)



```

```{r}
## Neighborhood plotted against SalePrice
plot(Neighborhood.id, hpmodel$SalePrice, xlab = "Neighborhood")
nreg.mod = lm(SalePrice ~ Neighborhood.id, data=hpmodel)
plot(allEffects(nreg.mod))
summary(nreg.mod)
```


Based off this chart, in an effort to simplify this 25-part categorical variable, I am going to aggregate all neighborhood values into 3 bins based off of their effect on SalePrice. Group 0 = Average, Group 1 = Good, Group 2 = Wealthy 

Group 0 = 10,11,18,3,4,8,20,19,13,12,2,1,17,21,15
Group 1 = 22,24,25,5,6,7,9
Group 2 = 14,16,23


```{r}

#### Assigning Group to Neighborhood.id

hpmodel$Neighborhood <- revalue(Neighborhood.id, c("1" = "0", "2" = "0", "3" = "0", "4" = "0", "5" = "1", "6" = "1", "7" = "1", "8" = "0", "9" = "1", "10" = "0", "11" = "0", "12" = "0", "13" = "0", "14" = "2", "15" = "0", "16" = "2", "17" = "0", "18" = "0", "19" = "0", "20" = "0", "21" = "0", "22" = "1", "23" = "2", "24" = "1", "25" = "1"))

### Change factor variables to factor
hpmodel$Neighborhood <- as.factor(hpmodel$Neighborhood)


```
    
## Part a) 

```{r}

## descriptive statistics
stat.desc(hpmodel)

## Changing -inf to NA
#is.na(hpmodel) <- sapply(hpmodel, is.infinite)


```


This table shows us the number of values, null values, missing values, the min-max, range, sum, median, mean, SE of mean, 95% CI, var, and std.dev of the 10 variables I selected. Non-numeric variables (Neighborhood, CentralAir has been given integer values as shown above).


```{r}
## Histogram of hpmodel
plot_histogram(hpmodel)

```


Here lies all the 9 histograms for our variables minus the two categorical variables. We can get a general idea of the distribution for each of our variables here (i.e. Most of the houses in our dataset involve GrLivArea between 1000-2000 sq ft, Most of the houses in our dataset was remodeled/built at or before 1950 or after 1990).


```{r}
## QQ plot of hpmodel
plot_qq(hpmodel, ncol = 4)

```


Ignoring Neighborhood and CentralAir, because they are factor variables, the meat of our data (IQR) within our timeline of study in regards to that variable (i.e. 1950-2010 for YearRemodAdd) aligns decently well with the theoretical normal distribution (the line). We say decently and not perfectly because we can see from the GrLivArea, OpenPorchSF, and SalePrice, the high end starts to diverge from the normal implying that as we get into higher end of square footage and price there seems to be some exponential growth behavior.


```{r}
## correlation of variables
plot_correlation(hpmodel)

```


Some interesting things we can find from this chart is that GrLivArea and TotRmsAbvGr are closely related, which we will probably need to address in the future (multicollinearity). The most important thing to notice is the last column, which outlines the correlation between our 10 variables and SalePrice, which is what we are trying to predict and model. From first glance, it appears OverallQual and GrLivArea may have the biggest effects on SalePrice.

```{r}
## boxplot of variables against SalePrice
plot_boxplot(hpmodel, by = "SalePrice")

```


Once again, from all these infographics we want to ignore Neighborhood/CentralAir as they are indicator variables. With that being said, right off the bat, we can visually see a generally positive correlation between TotalBsmtSF, GrLivingArea, OpenPorchSF, OverallQual, TotRmsAbvGrd, YearRemodAdd, GarageArea and SalePrice. This matches our intuition. 

```{r}
##scatterplot of variables against SalePrice
plot_scatterplot(hpmodel, by = "SalePrice", ncol = 4)

```


Just another way we can look at how our variables look plotted against SalePrice. Similar conclusions as boxplot, but it is clearer to see how the outliers mess with the overall positive trend (OpenPorchSF).


## Part b)


```{r}
###Using Box-Cox we can determine the best transformation for each non-indicator variable

## GarageArea
symbox(hpmodel$GarageArea, powers = c(-2,-1,-.5,0,0.5,1,2))

```


Log transformation appears to be the most normal looking distribution for GarageArea. However, given the cases where houses may not have garages, we will stick with a **linear** model.


```{r}

## GrLivArea
symbox(hpmodel$GrLivArea, powers = c(-2,-1,-.5,0,0.5,1,2))

```


The most "even" looking transformation appears to be a **log** transformation for GrLivArea.

```{r}

## OpenPorchSF
symbox(hpmodel$OpenPorchSF, powers = c(-2,-1,-.5,0,0.5,1,2))

```


Looks like log seems to be the best transformation for OpenPorchSF as well as it appears the most even. However, given the nature of the data and the large amount of 0's, I will opt for a **sqrt** transformation instead.

```{r}

## OverallQual
symbox(hpmodel$OverallQual, powers = c(-2,-1,-.5,0,0.5,1,2))

```


Although a quadratic has one less outlier outside the tails, the **linear** boxplot is atleast balanced on both ends so we will stick with that one for OverallQual.


```{r}

## TotalBsmtSF
symbox(hpmodel$TotalBsmtSF, powers = c(-1,-.5,0,0.5,1))

```

A **sqrt** transformation looks to be the most fitting for TotalBsmtSF.


```{r}

## TotRmsAbvGrd
symbox(hpmodel$TotRmsAbvGrd, powers = c(-1,-.5,0,0.5,1))

```


The **log** transformation appears to be the most normal of the 5 choices for TotRmsAbvGrd.


```{r}

## YearRemodAdd
symbox(hpmodel$YearRemodAdd, powers = c(-1,-.5,0,0.5,1))

```


**Linear** seems fine here as any transformation looks fine so we will keep it as it is.


```{r}

## LotArea
symbox(hpmodel$LotArea, powers = c(-1,-.5,0,0.5,1))

```


**Log** looks the best for LotArea as it looks the most evenly distributed.


```{r}

## SalePrice
symbox(hpmodel$SalePrice, powers = c(-1,-.5,0,0.5,1))

```

**Log** transformation for SalePrice seems to be the best fit.


```{r}
#### Transformations performed on the model (log+1 to keep variables as 0 instead of omission)


## Applying the log transformation to GrLivArea
hpmodel$GrLivArea <- log(hpmodel$GrLivArea)
## Applying the sqrt transformation to OpenPorchSF
hpmodel$OpenPorchSF <- sqrt(hpmodel$OpenPorchSF)
## Applying the sqrt transformation to TotalBsmtSF
hpmodel$TotalBsmtSF <- sqrt(hpmodel$TotalBsmtSF)
## Applying the log transformation to TotRmsAbvGrd
hpmodel$TotRmsAbvGrd <- log(hpmodel$TotRmsAbvGrd)
## Applying the log transformation to LotArea
hpmodel$LotArea <- log(hpmodel$LotArea)
## Applying the log transformation to SalePrice
hpmodel$SalePrice <- log(hpmodel$SalePrice)

```

```{r}
## Checking if transformation worked and transforming the negative inf values to NA then omitting them

#is.na(hpmodel) <- sapply(hpmodel, is.infinite)
#hpmodel.adj <- na.omit(hpmodel)
## Re-numbering rows after omission
#row.names(hpmodel.adj) <- 1:nrow(hpmodel.adj)

plot_boxplot(hpmodel, by = "SalePrice")
head(hpmodel)
```


Although still not great, the transformed variables help alleviate some of the issues that outliers were causing. Things look a lot more even. We can see a more clear trend among the transformed predictors than before.


## Part c)

```{r}
## Multiregression with data hpmodel
# hpregmod is the regression model

hpregmod <- lm(SalePrice ~ CentralAir + GarageArea + GrLivArea + Neighborhood + OpenPorchSF + OverallQual + LotArea + TotalBsmtSF + TotRmsAbvGrd + YearRemodAdd, data = hpmodel)
summary(hpregmod)

# Illustrates the effect of the different levels of CentralAir and Neighborhood more clearly 
effect("CentralAir",hpregmod, xlevels = 2)
effect("Neighborhood",hpregmod, xlevels = 3)
```

At first glance, we have statistical significance at the 5% level for all of our variables except TotRmsAbvGrd and the OpenPorchSF. This is quite interesting because that tells us there is no significant evidence that the amount of rooms in your house or porch size affects the sale price. My intuition initially told me there would be a correlation.

All other variables are statistically significant as of now;

CentralAir1 has an estimate of .199, which means that we can expect an increase in sales price of around 22% if there is a a central air conditioning unit in the house.

GarageArea has a coefficient of .0596, which means that for every 1% increase in garage square footage, we can expect on average a .0596% increase in sales price.

GrLivArea has a coefficient of .370, which means that for every 1% increase in above grade living area square footage, we can expect on average a .370% increase in sales price.

Neighborhood1 has a coefficient of .0689 which means that we can expect an increase in sales price of around 7.133% if the house lies in the areas designated as Group 1.

Likewise, Neighborhood2 has a coefficient of .1787, which means that we can expect an increase in sales price of around 19.566% if the house lies in the areas designated as Group 2.

OpenPorchSF has an estimate of 0.0148, which means that for every 1% increase in open porch square footage, we can expect an increase in sales price of around .0148%.

OverallQual has an estimate of .0841, which means that for every 1 increase in rating for the house's quality, we can expect an increase of 8.774% in the house's sale price.

LotArea has an estimate of .0891, which means that for every 1% increase in lot area square footage, we can expect an increase in sales price of around .0891%.

TotalBsmtSF has an estimate of .153, which means that for every 1% increase in total basement square footage, we can expect an increase in sales price of around .153%.

YearRemodAdd has an estimate of .00255, which means that for every year later the house has been constructed/remodelled we can expect an increase in around .255% in sales price. 


## Part d)

```{r}
## Plot of the residuals
plot(hpregmod$residuals, pch=20, ylim=c(-2,.7), xlim = c(0,1450))
abline(h=0, lwd=2, col = "blue")
# Using the Jarque-Bera test to see if our residuals resemble a normal distribution
jarque.bera.test(hpregmod$residuals)

```

Visually, our residual plot clearly shows some outliers that are extremely far from the rest of the residuals. Running the Jarque-Bera test, we can see that our x-squared is 12182 and our p-value is nearly 0, indicating our residuals do **not** resemble a normal distribution. We should find and eliminate outliers.

```{r}
# Find influential outliers using Cook's Distance
influencePlot(hpregmod)
cooksd <- cooks.distance(hpregmod)
plot(cooksd)
abline(h = 4/(nrow(hpmodel)-11), col = "red")
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/(nrow(hpmodel)-11), names(cooksd),""), col="red")
# Removing influential outliers using threshold of 4/N-k-1
influential <- as.numeric(names(cooksd)[(cooksd > 4/(nrow(hpmodel)-11))])
# New data set without outliers
hpmodeladj.screen <- hpmodel[-c(influential),] 


```


Here we eliminate some influential outliers using Cook's Distance and we will re-run the regression and take a look at residuals.

```{r}
## Multiregression with data screened for outliers; data = hpmodeladj.screen
# hpregmod.screen is the regression model

hpregmod.screen <- lm(SalePrice ~ CentralAir + GarageArea + GrLivArea + Neighborhood + OpenPorchSF + OverallQual + LotArea + TotalBsmtSF + TotRmsAbvGrd + YearRemodAdd, data = hpmodeladj.screen)
summary(hpregmod.screen)

# Illustrates the effect of the different levels of CentralAir and Neighborhood more clearly 
effect("CentralAir",hpregmod.screen, xlevels = 2)
effect("Neighborhood",hpregmod.screen, xlevels = 3)

```


Unsurprisingly, our adjusted R-squared went up with the removal of influential outliers. Our model better fits this subset of data points. The biggest change is our variable, TotRmsAbvGrd, suddenly became statistically significant with the removal of influential outliers.

```{r}
### Checking out the residuals for the screened model

## Plot of the residuals
plot(hpregmod.screen$residuals, pch=20, ylim=c(-.5,.5), xlim = c(0,1450))
abline(h=0, lwd=2, col = "blue")
# Using the Jarque-Bera test we can see if our residuals overall look good.
jarque.bera.test(hpregmod.screen$residuals)


```


Our residual plot looks a lot better with the removal of influential outliers; this is enforced by our Jarque-Bera test results of a x-squared of 1.1762 with a p-value of 0.5554. This means we can be confident our residuals resemble a normal distribution.


### Part e)

```{r}

## Mallows Cp test for which terms to keep in model
ss = regsubsets(SalePrice ~ CentralAir + GarageArea + GrLivArea + Neighborhood + OpenPorchSF + OverallQual + LotArea + TotalBsmtSF + TotRmsAbvGrd + YearRemodAdd, method=c("exhaustive"), nbest = 3, nvmax = 19, data = hpmodeladj.screen)

subsets(ss,min.size = 8, statistic="cp", legend=interactive())

```

Seems that the complete model is the one to keep as it has the lowest cp score.

```{r}
## Testing for multicollinearity, 1 is none, 1<x<5 is moderate >5 is high, we will go with 3 here

vif(hpregmod.screen)

```


Since GrLivArea's GVIF = 3.58, we are going to remove that variable due to multicollinearity. 


```{r}
## Multiregression with variable GrLivArea removed
# hpregmod.screen2 is the new regression model

hpregmod.screen2 <- lm(SalePrice ~ CentralAir + GarageArea + Neighborhood + OpenPorchSF + OverallQual + LotArea + TotalBsmtSF + TotRmsAbvGrd + YearRemodAdd, data = hpmodeladj.screen)
summary(hpregmod.screen2)

# Illustrates the effect of the different levels of CentralAir and Neighborhood more clearly 
effect("CentralAir",hpregmod.screen2, xlevels = 2)
effect("Neighborhood",hpregmod.screen2, xlevels = 3)

### hpregmod.screen2
```

### Part f)

```{r}
### Checking out the residuals for the new screened model

## Plot of the residuals
plot(hpregmod.screen2$residuals, pch=20, ylim=c(-.5,.5), xlim = c(0,1400))
abline(h=0, lwd=2, col = "blue")
# Using the Jarque-Bera test we can see if our residuals overall look good.
jarque.bera.test(hpregmod.screen2$residuals)
```


The residual plot looks good, evenly distributed across all ranges. Lower X-squared in the Jarque-Bera test as well and a p-value > .05 


### Part g)

```{r}
### Determine which model is better (screen or screen2)
## Akaike Information Criterion (AIC), the smaller the value of AIC, the better

AIC(hpregmod.screen, hpregmod.screen2)

```


Our original model hpregmod.screen is actually preferred over the model with GrLivArea removed.

```{r}
### Determine which model is better (screen or screen2)
## Bayesian Information Criterion (BIC), the smaller the better

BIC(hpregmod.screen, hpregmod.screen2)

```


The BIC seems to agree with the AIC that our original model with no variable taken out for multicollinearity is a better fit for the data. This could be because the degree with which GrLivArea was multicollinear wasn't high enough to hinder the predictive power it added when it was in the model. Since GrLivArea offered its own predictive capabilities, AIC and BIC seem to think it makes the original model better even though some multicollinearity may be involved.


### Part h)

```{r}
## Multiregression with possible interaction variables
# hpregmod.screen is the base regression model we will use since we decided it was better than screens2


#squaring regression to determine possible significant interactions
hpregmod.screen.interactions <- lm(SalePrice ~ (CentralAir + GarageArea + GrLivArea + Neighborhood + OpenPorchSF + OverallQual + LotArea + TotalBsmtSF + TotRmsAbvGrd + YearRemodAdd)^2, data = hpmodeladj.screen)
anova(hpregmod.screen.interactions)

# Illustrates the effect of the different levels of CentralAir and Neighborhood more clearly 
effect("CentralAir",hpregmod.screen.interactions, xlevels = 2)
effect("Neighborhood",hpregmod.screen.interactions, xlevels = 3)

```

Based off this summary, I would drop all interaction effects except those with very low p-value. I will keep OverallQual:TotalBsmtSF, GrLivArea:Neighborhood.

Therefore our new model will be built as such:

```{r}
## Multiregression with selected interaction variables
# New model wil be renamed hpregmod.screen.interactions.selected


#squaring regression to determine possible significant interactions
hpregmod.screen.interactions.selected <- lm(SalePrice ~ CentralAir + GarageArea + GrLivArea + Neighborhood + OpenPorchSF + OverallQual + LotArea  + TotRmsAbvGrd + TotalBsmtSF + YearRemodAdd + (OverallQual*TotalBsmtSF) + (GrLivArea*Neighborhood), data = hpmodeladj.screen)
summary(hpregmod.screen.interactions.selected)

# Illustrates the effect of the different levels of CentralAir and Neighborhood more clearly 
effect("CentralAir",hpregmod.screen.interactions.selected, xlevels = 2)
effect("Neighborhood",hpregmod.screen.interactions.selected, xlevels = 3)

```


At first glance, our model with interaction effects seem better because of the higher adjusted R-squared. To be more rigorous in our conclusion, we will test using the AIC and BIC.

```{r}
AIC(hpregmod.screen, hpregmod.screen2, hpregmod.screen.interactions.selected)
BIC(hpregmod.screen, hpregmod.screen2, hpregmod.screen.interactions.selected)
```


As expected, the new model with interaction variables has a lower AIC and BIC, indicating it is probably a better model for our data.



### Part i)

```{r}
### 5-fold cross-validation test

## Define Training Control

set.seed(123)
train.control <- trainControl(method = "cv", number = 5)

## Train the model
hpregmod.screen.interactions.selected <- train(SalePrice ~., data = hpmodeladj.screen, method = "lm", trControl = train.control)

## Summarize the results
print(hpregmod.screen.interactions.selected)
```


Based of the 5-fold cross-validation test, with a low RMSE and low MAE and high R-squared, the model performed well.

```{r}
## load test dataset
housepricemodel_test <- read.table("C:/Users/Matt/Downloads/test.csv", header=TRUE, sep = ",")


## keep variables desired
hpmodeltest <- subset(housepricemodel_test, select = c("GarageArea", "LotArea", "TotRmsAbvGrd", "Neighborhood", "OpenPorchSF", "YearRemodAdd", "GrLivArea", "TotalBsmtSF", "OverallQual", "CentralAir"))

### Change categorical variables to factor variables for - Neighborhood, CentralAir

## Analyzing Neighborhood vs price to see if I can simplify the variable into bins/fewer parts
Neighborhood.id <- revalue(hpmodeltest$Neighborhood, c("Blmngtn" = "1", "Blueste" = "2", "BrDale" = "3", "BrkSide" = "4", "ClearCr" = "5", "CollgCr" = "6", "Crawfor" = "7", "Edwards" = "8", "Gilbert" = "9", "IDOTRR" = "10", "MeadowV" = "11", "Mitchel" = "12", "NAmes" = "13", "NoRidge" = "14", "NPkVill" = "15", "NridgHt" = "16", "NWAmes" = "17", "OldTown" = "18", "SWISU" = "19", "Sawyer" = "20", "SawyerW" = "21", "Somerst" = "22", "StoneBr" = "23", "Timber" = "24", "Veenker" = "25"))

## CentralAir
hpmodeltest$CentralAir <- revalue(hpmodeltest$CentralAir, c("N" = "0", "Y" = "1"))

### Change variables to factor
Neighborhood.id<- as.factor(Neighborhood.id)
hpmodeltest$CentralAir <- as.factor(hpmodeltest$CentralAir)

#### Assigning Group to Neighborhood.id

hpmodeltest$Neighborhood <- revalue(Neighborhood.id, c("1" = "0", "2" = "0", "3" = "0", "4" = "0", "5" = "1", "6" = "1", "7" = "1", "8" = "0", "9" = "1", "10" = "0", "11" = "0", "12" = "0", "13" = "0", "14" = "2", "15" = "0", "16" = "2", "17" = "0", "18" = "0", "19" = "0", "20" = "0", "21" = "0", "22" = "1", "23" = "2", "24" = "1", "25" = "1"))

### Change variables to factor
hpmodeltest$Neighborhood <- as.factor(hpmodeltest$Neighborhood)

#### Transformations performed on the test model (linear transformations not listed (no code needed))

## Applying the log transformation to GrLivArea
hpmodeltest$GrLivArea <- log(hpmodeltest$GrLivArea)
## Applying the sqrt transformation to OpenPorchSF
hpmodeltest$OpenPorchSF <- sqrt(hpmodeltest$OpenPorchSF)
## Applying the sqrt transformation to TotalBsmtSF
hpmodeltest$TotalBsmtSF <- sqrt(hpmodeltest$TotalBsmtSF)
## Applying the log transformation to TotRmsAbvGrd
hpmodeltest$TotRmsAbvGrd <- log(hpmodeltest$TotRmsAbvGrd)
## Applying the log transformation to LotArea
hpmodeltest$LotArea <- log(hpmodeltest$LotArea)




## Test set prediction
prediction <- predict(hpregmod.screen.interactions.selected, newdata = hpmodeltest)
SalePrice.prediction <- exp(prediction)
summary(SalePrice.prediction)



```

List of predicted house sale price based on model built from training set.